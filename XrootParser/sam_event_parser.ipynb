{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a spreadsheet of data from sam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys,csv,string\n",
    "from csv import reader\n",
    "\n",
    "# input is in sys.argv[1]\n",
    "\n",
    "# if len(sys.argv) < 2:\n",
    "#     print (\"Need to enter filename for csv\")\n",
    "#     sys.exit(0)\n",
    "\n",
    "# filename = sys.argv[1]\n",
    "filename = \"SamEvents.csv\"\n",
    "    \n",
    "if not \"csv\" in filename:\n",
    "  print (\" need to enter a csv filename\")\n",
    "  sys.exit(1)\n",
    "\n",
    "file = open(filename,'r')\n",
    "data = []\n",
    "\n",
    "for line in reader(file,dialect='excel'):\n",
    "  data.append(line)\n",
    "\n",
    "headers = data[0]\n",
    "\n",
    "# pull out records for each process ID and parse\n",
    "\n",
    "info = {}\n",
    "for line in data[1:]:\n",
    "\n",
    "    mydict = {}\n",
    "    timestamp = line[0]\n",
    "    \n",
    "    # clean up ID #'s'\n",
    "    for field in range(0,len(headers)):\n",
    "        entry = line[field]\n",
    "        if headers[field] in [\"process_id\",\"file_id\",\"file_size\",\"kafka.timestamp\"] and len(entry)>0:\n",
    "            entry  = int(entry.replace(\",\",\"\"))\n",
    "        \n",
    "        if headers[field] == \"file_size\" and entry == \"\":\n",
    "            entry = 0\n",
    "        \n",
    "        mydict[headers[field]] = entry\n",
    "    \n",
    "    process = mydict[\"process_id\"] \n",
    "\n",
    "\n",
    "    if not process in info:\n",
    "        info[process] = {}\n",
    "    info[process][timestamp] = mydict\n",
    "    \n",
    "# now merge records for each jobid into a single record\n",
    "\n",
    "single = []\n",
    "\n",
    "keys = None\n",
    "for a  in info:\n",
    "    process = info[a]\n",
    "    \n",
    "    stamps = list(process.keys())\n",
    "    \n",
    "    file_id = \"\"\n",
    "    file_url = \"\"\n",
    "    ended  = False\n",
    "    file_size = 0\n",
    "    kafka_begin = 0\n",
    "    \n",
    "    # need to sort them by time and then pick out more recent data\n",
    "    \n",
    "    file_state = \"Unknown\"\n",
    "    process_state = \"Unknown\"\n",
    "    for i in sorted(stamps):\n",
    "        # info from all records\n",
    "        lasttime = i\n",
    "        record = process[i]\n",
    "        timestamp = i\n",
    "        event = record[\"event\"]\n",
    "        \n",
    "        if record[\"file_state\"]!= \"\":\n",
    "            file_state = record[\"file_state\"]\n",
    "        if record[\"process_state\"]!= \"\":\n",
    "            process_state = record[\"process_state\"]\n",
    "        process_state = record[\"process_state\"]\n",
    "        if record[\"file_size\"] != 0:\n",
    "            file_size =record[\"file_size\"]\n",
    "        #info just from the transfers\n",
    "        if record[\"file_state\"] not in [\"transferred\",\"consumed\"] and record[\"process_state\"] not in [\"completed\",\"finished\"]:\n",
    "            continue\n",
    "        if record[\"file_id\"] != \"\":\n",
    "            file_id =record[\"file_id\"]\n",
    "        if kafka_begin == 0:\n",
    "            kafka_begin = record[\"kafka.timestamp\"]\n",
    "        kafka_end = record[\"kafka.timestamp\"]\n",
    "            \n",
    "        if record[\"file_url\"] != \"\":\n",
    "            file_url = record[\"file_url\"]\n",
    "        if record[\"node\"] != \"\":\n",
    "            node = record[\"node\"]\n",
    "        if file_state == \"transferred\":\n",
    "            start_time = timestamp\n",
    "        if file_state == \"consumed\" or process_state in [\"completed\",\"finished\"] :\n",
    "            end_time = timestamp\n",
    "            ended = True\n",
    "    if not ended: \n",
    "        end_time = lasttime\n",
    "    # make the records out of the bits you collected\n",
    "    new = {}\n",
    "    new[\"job_id\"]=a\n",
    "    new[\"start\"] = start_time\n",
    "    new[\"end\"]= end_time\n",
    "    new[\"node\"]=node\n",
    "    new[\"file_id\"]=file_id\n",
    "    new[\"file_url\"]=file_url\n",
    "    new[\"file_size\"]=file_size\n",
    "    new[\"last_file_state\"]=file_state\n",
    "    new[\"last_process_state\"]=process_state\n",
    "    new[\"ended\"]=ended\n",
    "    new[\"kafka_begin\"]= kafka_begin\n",
    "    new[\"kafka_end\"]= kafka_end\n",
    "    new[\"kafka_alivetime\"]=(kafka_end-kafka_begin)/1000.\n",
    "    if file_size > 0:\n",
    "        single.append(new)\n",
    "   \n",
    "    \n",
    "keys = single[0].keys()        \n",
    "        \n",
    "# now dump to a csv file\n",
    "csv_file = \"SamTransfers.csv\"\n",
    "try:\n",
    "    with open(csv_file, 'w') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        for data in single:\n",
    "            \n",
    "            writer.writerow(data)\n",
    "            \n",
    "except IOError:\n",
    "    print(\"I/O error\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
